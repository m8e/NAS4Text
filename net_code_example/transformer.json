/// Transformer model settings (for de_en_iwslt).
/// Settings copied from T2T:
//
//@registry.register_hparams
//def transformer_small():
//  hparams = transformer_base()
//  hparams.num_hidden_layers = 2
//  hparams.hidden_size = 256
//  hparams.filter_size = 1024
//  hparams.num_heads = 4
//  return hparams

// Related argument options:
//
// --hparams transformer_de_en_iwslt

/// Result Network:
//
//ParalleledChildNet(
//  (module): ChildNet(
//    (encoder): ChildEncoder(
//      (embed_tokens): Embedding(24898, 256, padding_idx=0)
//      (embed_positions): LearnedPositionalEmbedding(256, 256, padding_idx=0)
//      (layer_0): SelfAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (layer_1): SelfAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (fc2): Linear(in_features=256, out_features=256)
//    )
//    (decoder): ChildDecoder(
//      (embed_tokens): Embedding(24898, 256, padding_idx=0)
//      (embed_positions): LearnedPositionalEmbedding(256, 256, padding_idx=0)
//      (layer_0): SelfAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (attention_0): EncDecAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (layer_1): SelfAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (attention_1): EncDecAttention(
//        (linears): ModuleList(
//          (0): Linear(in_features=256, out_features=256)
//          (1): Linear(in_features=256, out_features=256)
//          (2): Linear(in_features=256, out_features=256)
//          (3): Linear(in_features=256, out_features=256)
//        )
//        (dropout): Dropout(p=0.1)
//      )
//      (fc2): Linear(in_features=256, out_features=256)
//      (fc_last): Linear(in_features=256, out_features=24898)
//    )
//  )
//)

[
    [
        [2, 1],
        [2, 1]
    ],
    [
        [2, 1],
        [2, 1]
    ]
]
