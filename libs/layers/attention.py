#! /usr/bin/python
# -*- coding: utf-8 -*-

"""Attention layer.

Layer code:
[Attention, GlobalAttention?, ...]
"""

__author__ = 'fyabc'


def build_attention(layer_code, input_shape, hparams):
    """

    Args:
        layer_code:
        input_shape:
        hparams:

    Returns: layer, output_shape
    """

    raise NotImplementedError('Attention layer not implemented')
